\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage[normalem]{ulem}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\title{SE 3XA3: Test Plan\\OpenCameraRefined}

\author{Team \#211, CAMERACREW
		\\ Faisal Jaffer, jaffem1
		\\ Dominik Buszowiecki, buszowid
		\\ Pedram Yazdinia, yazdinip
		\\ Zayed Sheet, sheetz
}

\date{\today}


\begin{document}

\maketitle

\pagenumbering{roman}
\tableofcontents
\listoftables
\listoffigures

\begin{table}[bp]
\caption{\bf Revision History}
\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
February 29, 2020 & 1.0 & Initial Document\\
\date{\today} & 2.0 & Fixed feedback and added new test cases\\
\bottomrule
\end{tabularx}
\end{table}

\newpage

\pagenumbering{arabic}

\section{General Information}

\subsection{Purpose}
    The purpose of this document is to describe the process and techniques that will be used to verify the correctness of the system.

\subsection{Scope}
    The scope of the document is to describe the testing techniques that will be used to verify the additional features that will be implemented specifically. This includes \sout{performing unit testing, integrated testing and automated testing} \textcolor{red}{performing unit tests, integrated tests and automated tests} on any additional functions that have been implemented as well as on the system as a whole.

\subsection{Acronyms, Abbreviations, and Symbols}
	
\begin{table}[hbp]
\caption{ \sout{\textbf{Table of Abbreviations}}} \label{Table}

\begin{tabularx}{\textwidth}{p{3cm}X}
\toprule
\textbf{\sout{Abbreviation}} & \textbf{\sout{Definition}} \\
\midrule
\sout{Abbreviation1} & \sout{Definition1}\\
\sout{Abbreviation2} & \sout{Definition2}\\
\bottomrule
\end{tabularx}

\end{table}

\begin{table}[hbp]
\caption{ \textcolor{red}{\textbf{Table of Abbreviations}}} \label{Table}

\begin{tabularx}{\textwidth}{p{3cm}X}
\toprule
\textbf{\textcolor{red}{Abbreviation}} & \textbf{\textcolor{red}{Definition}} \\
\midrule
\textcolor{red}{ML} & \textcolor{red}{Machine Learning}\\
\textcolor{red}{TFLite} & \textcolor{red}{Tensorflow Lite}\\
\bottomrule
\end{tabularx}

\end{table}

\begin{table}[!htbp]
\caption{\textbf{Table of Definitions}} \label{Table}

\begin{tabularx}{\textwidth}{p{4cm}X}
\toprule
\textbf{Term} & \textbf{Definition}\\
\midrule
OpenCamera & A open source camera application for Android\\
OpenCameraRefined & Our teams version of the OpenCamera application\\
Gesture & A movement of a body part used to express something\\
Filter & The process of changing the appearance of an image by manipulating its colours and shades.\\
Machine Learning & The process of using computer software to find a relationship between a input and a output typically based on provided input output pairs.\\
TensorFlow & A open source machine learning library\\
Real Time & A system is real time if it responds to input immediately as it happens (in other words live)\\
\bottomrule
\end{tabularx}

\end{table}	

\subsection{Overview of Document}
    \sout{Our team} \textcolor{red}{The development team} will be developing two additional features to the open source camera app called OpenCamera.  The first feature will allow the user to take a photo through the use a gesture. The second feature allows the user to apply a photo filter either before or after the photo has been taken. This document will outline the tests to be performed for both of these features.

\section{Plan}
	
\subsection{Software Description}
This software is designed to change the way we capture pictures through our phones. Using image processing algorithms and machine learning, the team is focused on enabling the software to process and recognize a certain gesture in real time. In addition, the software is designed to offer the user with real time filters, mainly using existing packages such as OpenCV. Some initial gestures recognized by the software can include Smile, Thumbs up or Wave. Future gestures can be added by retraining the Object Classification algorithm through machine learning frameworks such as TensorFlow. The purpose is to gain accessibility and functionality through the use of “hands-free” picture capture and real time filters. The Software is developed using Android Studio and written prominently in Java. The user is also given the opportunity to change the configuration of the automatic image capture. For example, the user can mandate the application to only take a picture when a defined gesture is performed by the user for a certain period of time. 

\subsection{Test Team}
The following team members are responsible for scheduling, planning and executing different test phases, covering the entire code:
\begin{enumerate}
    \item Zayed Sheet
    \item Pedram Yazdinia
\end{enumerate}

\subsection{Automated Testing Approach}
The Testing process for each module is initiated by extracting the requirements that are affected by the test cases. These requirements should be recorded and verified. The team is then responsible to determine the testing level. The traditional testing levels include, Unit Testing, Integration Testing and System Testing. Developers group the tests into a certain level by looking at where they are added in the software development process or by checking how specific they are in the context of the entire implementation.  Once the testing level is determined, the developers brainstorm on different testing methods based on the scope of the testing. The testing methods mainly include White-box, Black-box or Grey-box testing which is a combination of the other two. In the execution stage, the team first compares different testing tools available, and selects the ones that are most compatible with our level and method of testing. Finally, the tests run, and the defects are one by one reviewed. In the final stages, it is crucial that the team updates the Requirements Traceability Matrix that shows the completeness of our validation through Software Testing. Finally, the developers are responsible to revise and review the effected requirements if an error has been fund. The implementation will change accordingly to the test cases results and the new requirements.  

\subsection{Testing Tools}
Open Camera is mainly developed using Android Studio and TensorFlow. Android Studio is the official IDE for android development which includes a comprehensive set of features used for unit testing, code coverage and debugging. As a result, the test team will mainly use Android Studio for testing purposes as well. 	Other tools such as Epxeritest and Robotinum were considered which offer a more advanced way to test the code, however for time consuming purposes we have decided to use Android Studio. 
		
\subsection{Testing Schedule}
		
See Gantt Chart at the following url \href{https://gitlab.cas.mcmaster.ca/yazdinip/opencamerarefined/-/blob/master/Doc/DevelopmentPlan/Gantt.png}{Gantt-Proj}


\section{System Test Description}
	
\subsection{Tests for Functional Requirements}

\subsubsection{Input through gestures}
		
\paragraph{Perform actions based on trained gestures}

\begin{enumerate}

\item{test-smile-capture\\}

Type: \sout{Manual}\textcolor{red}{Manual, Static, Functional}
					
Initial State: 
    ML model and label files in correct directory. Camera view is initialized. Preview width and height is known. Inception model is initialized. 
					
Input: Face in the view of the selected camera. (Image is the input)
					
Output: Green bounding box around the face in the preview. Following a successful detection, a picture is captured after 2 seconds. 
					
How test will be performed: A tester will hold the Android device running our version of the OpenCamera application and verify that a green bounding box is displayed around the face on the preview and a picture is captured 2 seconds after the user smiles in the view of the camera.

\item{test-face-detect\\}

Type: \sout{Manual}\textcolor{red}{Manual, Static, Functional}
					
Initial State: 
    ML model and label files in correct directory. Camera view is initialized. Preview width and height is known. Inception model is initialized. 
					
Input: Face in the view of the selected camera. (Image is the input)
					
Output: Green bounding box around the face in the preview. 
					
How test will be performed: A tester will hold the Android device running our version of the OpenCamera application and verify that a green bounding box is displayed around the face on the preview.

\item{test-thumb-filter\\}

Type: \sout{Manual}\textcolor{red}{Manual, Static, Functional}
					
Initial State: 
    ML model and label files in correct directory. Camera view is initialized. Preview width and height is known. Inception model is initialized. OpenCV is initialized. 
					
Input: "Thumbs Up" in the view of the selected camera. (Image is the input)
					
Output: Orange bounding box around the detected thumb following a filter displayed on the preview. 
					
How test will be performed: A tester will hold the Android device \textcolor{red}{, at a distance of 1 meter,} running our version of the OpenCamera application and verify that they are able to use the "live filter" when they show "thumbs up".

\end{enumerate}

\subsubsection{Live Filter}

\begin{enumerate}

\item{\sout{test-cycle-through-filters}\textcolor{red}{ test-cycle-filters}\\}

Type: \sout{Manual}\textcolor{red}{Manual, Static, Functional}
					
Initial State: 
    ML model and label files in correct directory. Camera view is initialized. Preview width and height is known. Inception model is initialized. OpenCV is initialized. 
					
Input: "Thumbs Up" in the view of the selected camera. (Image is the input)
					
Output: Every time a new "thumbs up" is detected, the system displays a new filter (cycles through the 4 filters available.)
					
How test will be performed: A tester will hold the Android device running our version of the OpenCamera application and verify that they are able to scroll through the "live filters" every time they show a "thumbs up".

\item{\textcolor{red}{ test-button-filter}\\}

Type: \textcolor{red}{Manual, Static, Functional}
					
\textcolor{red}{Initial State: 
    User has the Open Camera app open}
					
\textcolor{red}{Input: User touches the filter button.}
					
\textcolor{red}{Output: Filters are cycled through in the preview.}
					
\textcolor{red}{How test will be performed: A tester will hold the Android device running our version of the OpenCamera application and verify that upon touching the filter button, the user can toggle through all filters.}

\end{enumerate}

\subsubsection{\textcolor{red}{Modify Captured Image}}

\begin{enumerate}

\item{\textcolor{red}{ test-save-filter}\\}

Type: \textcolor{red}{Manual, Static, Functional}
					
\textcolor{red}{Initial State: 
    User has set the filter on preview.}
					
\textcolor{red}{Input: User smiles or touches the capture button)}
					
\textcolor{red}{Output: Image with the filter is saved}
					
\textcolor{red}{How test will be performed: A tester will hold the Android device running our version of the OpenCamera application and verify that upon taking an image in filter mode, the saved image has filter.}

\item{\textcolor{red}{ test-modify-picture}\\}

Type: \textcolor{red}{Manual, Static, Functional}
					
\textcolor{red}{Initial State: 
    User has captured the image and opens teh gallery.}
					
\textcolor{red}{Input: User chooses a modifying tool.}
					
\textcolor{red}{Output: User is able to modify the captured image.}
					
\textcolor{red}{How test will be performed: A tester will hold the Android device running our version of the OpenCamera application and verify that upon taking an image, the gallery offers them a tool to modify the image with.}

\end{enumerate}

\subsubsection{\textcolor{red}{Simple Capture}}

\begin{enumerate}

\item{\textcolor{red}{ test-save-picture}\\}

Type: \textcolor{red}{Manual, Static, Functional}
					
\textcolor{red}{Initial State: 
    User has opened the OpenCamera application.}
					
\textcolor{red}{Input: User touches capture button}
					
\textcolor{red}{Output: Image with the filter is saved}
					
\textcolor{red}{How test will be performed: A tester will hold the Android device running our version of the OpenCamera application and verify that upon touching the capture button, the application captures an image.}

\item{\textcolor{red}{ test-switch-camera}\\}

Type: \textcolor{red}{Manual, Static, Functional}
					
\textcolor{red}{Initial State: 
    User has opened the OpenCamera application.}
					
\textcolor{red}{Input: User switched the cameras}
					
\textcolor{red}{Output: Camera view switches}
					
\textcolor{red}{How test will be performed: A tester will hold the Android device running our version of the OpenCamera application and verify that upon switching camera views, the filters remain on view.}

\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

\subsubsection{\textcolor{red}{Usability}}
\begin{enumerate}

\item{\textcolor{red}{test-ease-of-use}\\}
Type: \textcolor{red}{Manual, Static, Functional}
					
\textcolor{red}{Initial State: User has opened the OpenCamera application.}
				
\textcolor{red}{Input/Condition: User is told: "smiling will automatically capture a photo, and a thumbs up will switch between filters"}
					
\textcolor{red}{Output/Result: The system is able to detect the gesture and perform relevant action.}
					
\textcolor{red}{How test will be performed: A user will be selected, who hasn't used this application before, and will be instructed with the following: "smiling will automatically capture a photo, and a thumbs up will switch between filters". Test will pass once the user can successfully perform these actions.}

\item{\textcolor{red}{test-ease-of-learning}\\}
Type: \textcolor{red}{Manual, Static, Functional}
					
\textcolor{red}{Initial State: User has opened the OpenCamera application.}
				
\textcolor{red}{Input/Condition: User smiles or shows thumbs up.}
					
\textcolor{red}{Output/Result: The user will rate the difficulty of using the application from 1-10}
					
\textcolor{red}{How test will be performed: five users (family members and friends who haven't used this application) will be selected and tested for difficulty of using the application before given any instructions. They will then rate the difficulty from a scale of 1-10}

\item{\textcolor{red}{test-look-and-feel}\\}
Type: \textcolor{red}{Manual, Static, Functional}
					
\textcolor{red}{Initial State: User has opened the OpenCamera application.}
				
\textcolor{red}{Input/Condition: the application UI is rendered }
					
\textcolor{red}{Output/Result: User will confirm or deny the app has a common "look and feel" of a typical camera application}
					
\textcolor{red}{How test will be performed: 5 users (family members and friends) will be selected to view the applications UI, and will be asked if the camera application has the typical look and feel of a camera application. They will then confirm or deny this, and their response will be recorded}
\end{enumerate}



\subsubsection{Performance \sout{of gesture detection}}

\begin{enumerate}

\item{test-reliability\\}

Type: \sout{Manual}\textcolor{red}{Manual, Static, Functional}
					
Initial State: Application being operated in an poorly lit environment
				
Input/Condition: A poorly lit gesture is found in the image.
					
Output/Result: The system is able to detect the gesture and perform relevant action.
					
How test will be performed: A tester will hold the Android device in a poorly lit room running our version of the OpenCamera application and verify that the application is able to perform the corresponding actions relevant to the gesture. \textcolor{red}{In this case test-smile-capture, test-face-detect and test-thumb-filter should pass.}
					
\item{test-speed\\}

Type: \sout{Dynamic}\textcolor{red}{Manual, Dynamic, Functional}
					
Initial State: ML model and label files in correct directory. Camera view is initialized. Preview width and height is known. Inception model is initialized. 
					
Input: Bitmap image of a gesture
					
Output: Detection of the gesture in the image under 200ms.
					
How test will be performed: After initializing the Inception model, we feed in an image with a gesture. Then measure the time it takes for the model to make a classification.

\item{\textcolor{red}{ test-garbage}\\}

Type: \textcolor{red}{Manual, Static, Functional}
					
\textcolor{red}{Initial State: 
    User has opened the OpenCamera application.}
					
\textcolor{red}{Input: User points camera at random objects}
					
\textcolor{red}{Output: Only smiles and thumbs up are the only 2 classes detected }
					
\textcolor{red}{How test will be performed: A tester will hold the Android device running our version of the OpenCamera application and verify that upon pointing the camera at different objects, only the trained objects are classified.}

\end{enumerate}

\subsubsection{\textcolor{red}{Operational and Environment}}
\begin{enumerate}

\item{\textcolor{red}{test-different-devices}\\}
Type: \textcolor{red}{Manual, Static, Functional}
					
\textcolor{red}{Initial State: User has opened the OpenCamera application.}
				
\textcolor{red}{Input/Condition: User performs the actions in the Functional Requirements.}
					
\textcolor{red}{Output/Result: The user should be able to successfully use the features.}
					
\textcolor{red}{How test will be performed: A tester will perform this test on devices with the following configurations: [5 inch phone running Android API 23, 5.5 inch phone running Android API 25 and 6 inch phone running Android API 27]. The tester shall confirm the successful use of all features.}

\end{enumerate}

\subsubsection{\textcolor{red}{Maintainability}}
\begin{enumerate}

\item{\textcolor{red}{test-newer-versions}\\}
Type: \textcolor{red}{Manual, Static, Functional}
					
\textcolor{red}{Initial State: User has opened the OpenCamera application.}
				
\textcolor{red}{Input/Condition: User performs the actions in the Functional Requirements.}
					
\textcolor{red}{Output/Result: The user should be able to successfully use the features.}
					
\textcolor{red}{How test will be performed: A tester will perform this test on devices with the following configurations: [5 inch phone running Android API 23, 5.5 inch phone running Android API 25 and 6 inch phone running Android API 27]. The tester shall confirm the successful use of all features.}

\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

\subsubsection{Functional Requirements}

\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 FR Trace & Test ID \\ 
  \hline
   \hline
\sout{BE1.1}\textcolor{red}{REQ1. REQ2. REQ12.} & test-smile-capture \\ 
  \hline
\sout{BE1.2}\textcolor{red}{REQ5.} & test-thumb-filter \\ 
 \hline
 \textcolor{red}{REQ5.} & test-face-detect \\ 
 \hline
 \textcolor{red}{REQ7.} & test-cycle-filters \\ 
 \hline
 \textcolor{red}{REQ6.} & test-button-filter \\ 
 \hline
 \textcolor{red}{REQ8. REQ9. REQ10.} & test-save-filter \\ 
 \hline
 \textcolor{red}{REQ4.} & test-modify-picture \\ 
 \hline
 \textcolor{red}{REQ11. REQ3. REQ13.} & test-save-picture \\ 
 \hline
 \textcolor{red}{REQ10.} & test-switch-camera \\ 
 \hline
\end{tabular}
\end{center}

\subsubsection{Non-Functional Requirements}
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 NFR Trace & Test ID \\ 
  \hline
   \hline
\textcolor{red}{EU} & \textcolor{red}{test-ease-of-use} \\ 
  \hline
\textcolor{red}{LR} & \textcolor{red}{test-ease-of-learning} \\ 
  \hline
  \textcolor{red}{ARL} & \textcolor{red}{test-look-and-feel} \\ 
  \hline
\textcolor{red}{SL. PA.} & \textcolor{red}{test-speed} \\ 
  \hline
\textcolor{red}{RR} & \textcolor{red}{test-garbage} \\ 
  \hline
\textcolor{red}{EP, PRE} & \textcolor{red}{test-different-devices} \\ 
  \hline
\textcolor{red}{ARM, ARS, MR} & \textcolor{red}{test-newer-versions} \\ 
 \hline
\end{tabular}
\end{center}
\section{Tests for Proof of Concept}

\subsection{Area of Testing}
		
\paragraph{\sout{Title for Test}}

\begin{enumerate}

\item{\sout{test-id1}\\}

\sout{Type: Functional, Dynamic, Manual, Static etc.}
					
\sout{Initial State: }
					
\sout{Input: }
					
\sout{Output: }
					
\sout{How test will be performed: }
					
\item{\sout{test-id2}\\}

\sout{Type: Functional, Dynamic, Manual, Static etc.}
					
\sout{Initial State: }
					
\sout{Input: }
					
\sout{Output: }
					
\sout{How test will be performed: }

\textcolor{red}{Proof of concept testing will primarily be focused on validating the detection of a smiling face and thumbs. These are the core requirements in our project, on which the other features are built on top. Therefore, majority of proof of concept testing will revolve around the validation of the detections in different situations. }

\end{enumerate}

	
\section{\sout{Comparison to Existing Implementation}}
				
\section{Unit Testing Plan}
% Unit testing for this project will be done in a JAVA environment using the JUnit 5 library.
		
\subsection{Unit testing of internal functions} 

\hspace{\parindent} Internal functions that return, filter or parse data and have objective outputs which can be manually predicted or derived will be tested in the Java environment using the JUnit 5 library. This will involve testing individual methods by providing them with an input or data to be used, and comparing the output with the expected output.

Internal functions that don't have objective outputs will be tested manually by making sure the output makes sense.

The inputs used to test these methods will either be proper data that is expected to be used in a real-life scenario, or improper/unexpected data that is meant to test the robustness of the program.

This project does not require any outside drivers, or stubs to be installed for the purposes of testing. All required drivers or stubs will already be available within the individual classes or project as a whole.

In terms of the metrics used to ensure adequate testing of internal functions, we will be using coverage metrics with a goal of 75\% code coverage
		
\subsection{Unit testing of output files}
\hspace{\parindent}Output files will be manually tested by making sure they make sense. This means they will visually be looked over by software testers to be approved.

For the filter output pictures, we will have example photos of what it should look like and compare it to the output. For gesture based actions we will have a user perform the gesture in a natural manner and ensure the software captures the gesture.

Per filter and gesture, we will take 10 or more pictures, using at least three different users and ensuring at least one photo is taken in a dark background environment and one in a light background environment. In addition, these tests will take place with all \sout{four} \textcolor{red}{software} test developers present to ensure the picture quality adheres to the standards of all \sout{four} \textcolor{red}{software} testers. Each filter and gesture must have a 100\% success rate, meaning all tests will have the approval of all \sout{four} \textcolor{red}{software} testers.

\bibliographystyle{plainnat}

\bibliography{SRS}

\end{document}
